{
    "databases":["WOS"],
    "limit":9999,
    "limit_per_database":null,
    "number_of_papers":27,
    "number_of_papers_by_database":{
        "WOS":27
    },
    "papers":[
        {
            "abstract":"Edge Computing pushes cloud capabilities to the edge of the network, closer to the users, to address stringent Quality-of-Service requirements and ensure more efficient bandwidth usage. Function-as-a-Service appears to be the most natural service model solution to enhance Edge Computing applications' deployment and responsiveness. Unfortunately, the conventional FaaS model does not fit well in distributed and heterogeneous edge environments, where traffic demands arrive to (and are served by) edge nodes that may get overloaded under certain traffic conditions or where the access points of the network might frequently change, as for mobile applications. This short paper tries to fill this gap by proposing DFaaS, a novel decentralized FaaS-based architecture designed to autonomously balance the traffic load across edge nodes belonging to federated Edge Computing ecosystems. DFaaS implementation relies on an overlay peer-to-peer network and a distributed control plane that takes decisions on load redistribution. Although preliminary, results confirm the feasibility of the approach, showing that the system can transparently redistribute the load across edge nodes when they become overloaded.",
            "authors":["Pelle, I.","Czentye, J.","DÃ³ka, J.","Sonkoly, B."],
            "categories":null,
            "citations":1,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/00000011.2222222",
            "keywords":["Greengrass","Lambda","IoT","Edge","Serverless","Cloud","AWS"],
            "number_of_pages":2,
            "pages":"33-34",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380485",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"Proceedings of the SIGCOMM 2020 Poster and Demo Sessions, SIGCOMM 2020"
            },
            "publication_date":"2021-08-10",
            "selected":null,
            "title":"DFaaS: Decentralized Function-as-a-Service for Federated Edge Computing",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115694621&origin=inward"],
            "id":"cc74e78b-4eb1-48d8-9429-8430880948b9"
        },
        {
            "abstract":"The Function-as-a-Service (FaaS) paradigm has a lot of potential as a computing model for fog environments comprising both cloud and edge nodes. When the request rate exceeds capacity limits at the edge, some functions need to be offloaded from the edge towards the cloud.In this position paper, we propose an auction-based approach in which application developers bid on resources. This allows fog nodes to make a local decision about which functions to offload while maximizing revenue. For a first evaluation of our approach, we use simulation.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.3409493",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2020-08-14",
            "selected":null,
            "title":"Towards Auction-Based Function Placement in Serverless Fog Platforms",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"3766473c-24c6-4ba8-8d9b-770b7b503f3c"
        },
        {
            "abstract":"Serverless Computing and Function as a Service (FaaS) is gaining traction in cloud-based application architectures used by startups and matured organizations alike. Organizations that are keen to leverage modern technology to gain a disruptive edge, optimal efficiency, advanced agility and save cost are adopting these architectural styles rapidly. Cloud service provider offer and dynamically manages the allocation of machine resources in serverless computing. The serverless architectures allows the developers to focus on business logic exclusively without worrying about preparing the runtime, managing deployment and infrastructure related concerns. FaaS may be assumed as a subset of Serverless Computing, in which, instead of coding a full-fledged cloud based application, the developer just writes (often small) functions which are piece of code (in one of the multiple programming languages supported by the platform) dedicated to do a focused, often single task that are invoked by triggers. It offers dynamic allocation and scaling of the resources and innovative trigger based costing model. This paper introduces Serverless Computing, and Function as a Service (FaaS), explores its advantages and limitations, options available with popular cloud and Platform as a Service (PaaS) providers, and emerging use cases and success stories.",
            "authors":["Pelle, I.","Czentye, J."],
            "categories":null,
            "citations":12,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/5897.8521479",
            "keywords":["Greengrass","Lambda","IoT"],
            "number_of_pages":2,
            "pages":"33-34",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380485",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"exemplo"
            },
            "publication_date":"2018-08-10",
            "selected":null,
            "title":"Winning in the era of Serverless Computing and Function as a Service",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115694621&origin=inward"],
            "id":"cc74e78b-4eb1-48d8-9429-8430880947b7"
        },
        {
            "abstract":"Fog computing extends multi-cloud computing by enabling services or application functions to be hosted close to their data sources. To take advantage of the capabilities of fog computing, serverless and the function-as-a-service (FaaS) software engineering paradigms allow for the flexible deployment of applications on multi-cloud, fog, and edge resources. This article reviews prominent fog computing frameworks and discusses some of the challenges and requirements of FaaS-enabled applications. Moreover, it proposes a novel framework able to dynamically manage multi-cloud, fog, and edge resources and to deploy data-intensive applications developed using the FaaS paradigm. The proposed framework leverages the FaaS paradigm in a way that improves the average service response time of data-intensive applications by a factor of three regardless of the underlying multi-cloud, fog, and edge resource infrastructure.",
            "authors":["Pelle, I.","Czentye, J.","DÃ³ka, J.","Sonkoly, B."],
            "categories":null,
            "citations":1,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405837.3411381",
            "keywords":["Greengrass","Lambda","IoT","Edge","Serverless","Cloud","AWS"],
            "number_of_pages":2,
            "pages":"33-34",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380485",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"Proceedings of the SIGCOMM 2020 Poster and Demo Sessions, SIGCOMM 2020"
            },
            "publication_date":"2021-08-10",
            "selected":null,
            "title":"PrEstoCloud: A Novel Framework for Data-Intensive Multi-Cloud, Fog, and Edge Function-as-a-Service Applications",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115694621&origin=inward"],
            "id":"cc74e78b-4eb1-48d8-9429-8430555558b9"
        },
        {
            "abstract":"The Function-as-a-Service (FaaS) paradigm has a lot of potential as a computing model for fog environments comprising both cloud and edge nodes, as compute requests can be scheduled across the entire fog continuum in a fine-grained manner. When the request rate exceeds capacity limits at the resource-constrained edge, some functions need to be offloaded toward the cloud. In this article, we present an auction-inspired approach in which application developers bid on resources while fog nodes decide locally which functions to execute and which to offload in order to maximize revenue. Unlike many current approaches to function placement in the fog, our approach can work in an online and decentralized manner. We also present our proof-of-concept prototype AuctionWhisk that illustrates how such an approach can be implemented in a real FaaS platform. Through a number of simulation runs and system experiments, we show that revenue for overloaded nodes can be maximized without dropping function requests.",
            "authors":["Pelle, I.","Czentye, J.","DÃ³ka, J.","Sonkoly, B."],
            "categories":null,
            "citations":1,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405837.0000000",
            "keywords":["Greengrass","Lambda","IoT","Edge","Serverless","Cloud","AWS"],
            "number_of_pages":2,
            "pages":"33-34",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380485",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"Proceedings of the SIGCOMM 2020 Poster and Demo Sessions, SIGCOMM 2020"
            },
            "publication_date":"2022-08-10",
            "selected":null,
            "title":"AuctionWhisk: Using an auction-inspired approach for function placement in serverless fog platforms",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115694621&origin=inward"],
            "id":"cc74e78b-4eb1-48d8-9429-8430222228b9"
        },
        {
            "abstract":"Although smart devices markets are increasing their sales figures, their computing capabilities are not sufficient to provide good-enough-quality services. This paper proposes a solution to organize the devices within the Cloud-Edge Continuum in such a way that each one, as an autonomous individual -Agent-, processes events/data on its embedded compute resources while offering its computing capacity to the rest of the infrastructure in a Function-as-a-Service manner. Unlike other FaaS solutions, the described approach proposes to transparently convert the logic of such functions into task-based workflows backing on task-based programming models; thus, agents hosting the execution of the method generate the corresponding workflow and offloading part of the workload onto other agents to improve the overall service performance. On our prototype, the function-to-workflow transformation is performed by COMPSs; thus, developers can efficiently code applications of any of the three envisaged computing scenarios - sense-process-actuate, streaming and batch processing - throughout the whole Cloud-Edge Continuum without struggling with different frameworks specifically designed for each of them.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.3333333",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2021-08-14",
            "selected":null,
            "title":"Colony: Parallel Functions as a Service on the Cloud-Edge Continuum",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"3766473c-24c6-4ba8-8d9b-123b7b503f3c"
        },
        {
            "abstract":"Fog computing can support IoT services with fast response time and low bandwidth usage by moving computation from the cloud to edge devices. However, existing fog computing frameworks have limited flexibility to support dynamic service composition with a data-oriented approach. Function-as-a-Service (FaaS) is a promising programming model for fog computing to enhance flexibility, but the current event- or topic-based design of function triggering and the separation of data management and function execution result in inefficiency for data-intensive IoT services. To achieve both flexibility and efficiency, we propose a data-centricprogramming model called Fog Function and also introduce its underlying orchestration mechanism that leverages three types of contexts: data context, system context, and usage context. Moreover, we showcase a concrete use case for smart parking where Fog Function allows service developers to easily model their service logic with reduced learning efforts compared to a static service topology. Our performance evaluation results show that the Fog Function can be scaled to hundreds of fog nodes. Fog Function can improve system efficiency by saving 95% of the internal data traffic over cloud function and it can reduce service latency by 30% over edge function.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.1234567",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2019-08-14",
            "selected":null,
            "title":"Fog Function: Serverless Fog Computing for Data Intensive IoT Services",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"3766473c-28b7-4ba8-8d9b-770b7b503f3c"
        },
        {
            "abstract":"Offoading computation from user devices to nodes with processing capabilities at the edge of the network is a major trend in today's network/service architectures. This work provides a broad view on the options available for supporting stateful function as a service.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.7654321",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2022-08-14",
            "selected":null,
            "title":"In-Network Computing With Function as a Service at the Edge",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"3766473c-24c6-4ab8-8d9b-770b7b503f3c"
        },
        {
            "abstract":"Serverless computing is an emerging cloud deployment model where developers can concentrate on developing application logic without worrying about the underlying architecture. It is similar to the platform as a service (PaaS) but at the functional level. Applications are usually deployed in the form of a set of functions independently and each function may be executed at separate servers thus also named as function as a service (FaaS). Serverless at the edge can handle thousands of concurrent functions invocations to process various kinds of events generated from resources like database, system logs, and other storage units, etc. A number of serverless frameworks like Openfaas, Openwhisk, Microsoft Azure, Amazon AWS allow dynamic scaling to handle the parallel request of stateless functions from the client-side. A separate container manager may be provisioned to handle distributed load for data processing. In this paper, we have evaluated the performance of serverless frameworks for parallel loads in terms of response time and throughput. In this paper, we have shown that the serverless framework is suitable for handling dynamic applications that can be executed on a number of stateless functions. An extensive comparison of the performance of serverless frameworks in handling concurrent invocations in terms of response time and throughput is also presented. It has been observed that Openwhisk is found to be the better serverless framework in terms of elasticity and scalability.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.1468752",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2021-08-14",
            "selected":null,
            "title":"Evaluation of Integrated Frameworks for Optimizing QoS in Serverless Computing",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"3123456c-24c6-4ba8-8d9b-770b7b503f3c"
        },
        {
            "abstract":"Serverless computing has recently emerged as a new execution model for cloud computing, in which service providers offer compute runtimes, also known as Function-as-a-Service (FaaS) platforms, allowing users to develop, execute and manage application functionalities. Following the rapid adoption of FaaS technologies and the introduction of numerous self hosted FaaS systems, the need for real time monitoring and scheduling of functions in an ecosystem of providers has become critical. In this paper, we present MPSC, a framework for supporting Multi-Provider Serverless Computing. MPSC monitors the performance of serverless providers in real time, and schedules applications across these providers. In addition, MPSC also provides APIs for users to define their own scheduling algorithms. When compared to scheduling on a single cloud resource MPSC provides a 4X speedup across multiple providers in a volatile edge computing environment.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.0833647",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2018-08-14",
            "selected":null,
            "title":"Supporting Multi-Provider Serverless Computing on the Edge",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"654321c-24c6-4ba8-8d9b-770b7b503f3c"
        },
        {
            "abstract":"In function as a service (FaaS), an application is decomposed into functions. We propose to generalize FaaS by allowing functions to alternate between remote-state and local-state phases, depending on internal and external conditions, and dedicating a container with persistent memory to functions when in a local-state phase.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.1122334",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2022-08-14",
            "selected":null,
            "title":"Stateful Function as a Service at the Edge",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"000000d-24c6-4ba8-8d9b-770b7b503f3c"
        },
        {
            "abstract":"Edge computing has emerged as a new paradigm to bring cloud applications closer to users for increased performance. Unlike back-end cloud systems which consolidate their resources in a centralized data center location with virtually unlimited capacity, edge-clouds comprise distributed resources at various computation spots, each with very limited capacity. In this article, we consider Function-as-a-Service (FaaS) edge-clouds where application providers deploy their latency-critical functions to process user requests with strict response time deadlines. In this setting, we investigate the problem of resource provisioning and allocation. After formulating the optimal solution, we propose resource allocation and provisioning algorithms across the spectrum of fully-centralized to fully-decentralized. We evaluate the performance of these algorithms in terms of their ability to utilize CPU resources and meet request deadlines under various system parameters. Our results indicate that practical decentralized strategies, which require no coordination among computation spots, achieve performance that is close to the optimal fully-centralized strategy with coordination overheads.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.0808080",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2022-08-14",
            "selected":null,
            "title":"Resource Provisioning and Allocation in Function-as-a-Service Edge-Clouds",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"3766473c-24c6-4ba8-9r5t-770b7b503f3c"
        },
        {
            "abstract":"With the establishment of the Everything-as-aService (XaaS) paradigm for service provisioning, coupled with the increasingly-demanding requirements imposed by modern network services, the need for a XaaS-aware orchestration system able to cope with a heterogeneous infrastructure, such as the one of Fog Computing environments, is evident. In this work, we describe the working principles and implementation aspects that allow the orchestration of services offered according to the Function-as-a-Service (FaaS) model. The live demonstration will showcase the ability of the system to deploy this kind of services on a suitable test bed, with comments on the procedure and the performance.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.9873215",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"Function-as-a-Service Orchestration in Fog Computing Environments"
            },
            "publication_date":"2022-08-14",
            "selected":null,
            "title":"Function-as-a-Service Orchestration in Fog Computing Environments",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"3766473c-24c6-4ba8-8d9b-770b7b874f3c"
        },
        {
            "abstract":"Serverless computing, or Function as a Service (FaaS), represents a research trend where applications are built and deployed as a group of stateless functions. Although initially proposed for the cloud, serverless computing has also found its place on Internet of Things (IoT) while bringing functions closer to the devices, in order to reduce latency and avoid unnecessary energy and resource consumption. is interesting that solutions can work in an integrated manner on edge, fog, and cloud layers. Mission-critical functions can be executed on edge and fog in order to benefit from low-latency responses, while heavy functions can be executed on the cloud to process huge amount of data produced by IoT sensors, as long as Internet connection is available. Existing surveys focus on serverless computing for specific layers and do not address a broad, integrated, and systematic vision regarding how IoT benefits from serverless on edge, fog, and cloud. With this in mind, this paper provides a comprehensive Systematic Literature Review that, after the selection process, covers 60 papers on the field of serverless computing for IoT on the three layers. This gives us insights about how functions are offloaded to different devices and how they interact with each other. We bring main components employed to incubate and execute functions, as well as the main challenges and open questions for this subject. Protocols, programming languages, and storage services related to the solutions are also presented. Finally, we show a rich taxonomy summarizing all characteristics in a single figure, along with a discussion about the overall architecture of serverless applications for IoT. We conclude that serverless computing is a promising technology for IoT applications, but several improvements still need to be made to popularize this concept and make it easier to use. (c) 2021 Elsevier B.V. All rights reserved",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.7418526",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"Serverless computing for Internet of Things: A systematic literature review"
            },
            "publication_date":"2022-08-14",
            "selected":null,
            "title":"Serverless computing for Internet of Things: A systematic literature review",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"3766473c-24c4-4ba8-8d9b-770b7b503f3c"
        },
        {
            "abstract":"Deployment of computing infrastructures at the edge of the network will drive a revolution in integrated solutions for smart mobility in the cities of the future, thanks to the promises of reduced latency and outbound traffic. The adoption of serverless computing will help realising this vision since it simplifies management while at the same time providing the application developers with a neat and clean Function-as-a-Service (FaaS) programming model. Today FaaS relies on HTTP over TCP, but QUIC is emerging fast as a replacement because it is more robust to packet losses and it allows connection roaming: both these advantages are especially important for mobile scenarios. In this paper we report the results of a preliminary evaluation of QUIC+HTTP/3 when used instead of TCP+HTTP within a framework for decentralized dispatching of FaaS function invocations, which shows that this direction is promising and deserves to be delved further in the future.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.3692584",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"A Preliminary Evaluation of QUIC for Mobile Serverless Edge Applications"
            },
            "publication_date":"2021-08-14",
            "selected":null,
            "title":"A Preliminary Evaluation of QUIC for Mobile Serverless Edge Applications",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"3766473c-24c6-4ba8-8d9b-777p7b503f3c"
        },
        {
            "abstract":"The exceedingly exponential-growing data rate highlighted numerous requirements and several approaches have been released to maximize the added-value of cloud and edge resources. Whereas data scientists utilize algorithmic models in order to transform datasets and extract actionable knowledge, a key challenge is oriented towards abstracting the underline layers: the ones enabling the management of infrastructure resources and the ones responsible to provide frameworks and components as services. In this sense, the serverless approach features as the novel paradigm of new cloud-related technology, enabling the agile implementation of applications and services. The concept of Function as a Service (FaaS) is introduced as a revolutionary model that offers the means to exploit serverless offerings. Developers have the potential to design their applications with the necessary scalability in the form of nanoservices without addressing themselves the way the infrastructure resources should be deployed and managed. By abstracting away the underlying hardware allocations, the data scientist concentrates on the business logic and critical problems of Machine Learning (ML) algorithms. This paper introduces an approach to realize the provision of ML Functions as a Service (i.e., ML-FaaS), by exploiting the Apache OpenWhisk event-driven, distributed serverless platform. The presented approach tackles also composite services that consist of single ones i.e., workflows of ML tasks including processes such as aggregation, cleaning, feature extraction, and analytics; thus, reflecting the complete data path. We also illustrate the operation of the approach mentioned above and assess its performance and effectiveness exploiting a holistic, end-to-end anti-fraud detection machine learning pipeline.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.9512367",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"Leveraging the serverless paradigm for realizing machine learning pipelines across the edge-cloud continuum"
            },
            "publication_date":"2021-08-14",
            "selected":null,
            "title":"Leveraging the serverless paradigm for realizing machine learning pipelines across the edge-cloud continuum",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"3766473c-24c6-4ba8-8d9b-770b7b506f6c"
        },
        {
            "abstract":"Scalable application development is highly influenced by two major trends - serverless computing and continuum computing. These trends have had little intersection, as most application architectures, even when following a microservices or function-based approach, are built around rather monolithic Function-as-a-Service engines that do not span continuums. Functions are thus separated codewise but not infrastructure-wise, as they continue to run on the same single platform they have been deployed to. Moreover, developing and deploying distributed applications remains non-trivial and is a hurdle for enhancing the capabilities of mobile and sensing domains. To overcome this limitation, the concept of self-balancing architectures is introduced in which liquid functions traverse cloud and edge/fog platforms in a continuum as needed, represented by the abstract notion of pressure relief valves based on resource capacities, function execution durations and optimisation preferences. With CoRFu, a reference implementation of a continuum-wide distributed Function-as-a-Service engine is introduced and combined with a dynamic function offloading framework. The implementation is validated with a sensor data inference and regression application.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.8742361",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2021-08-14",
            "selected":null,
            "title":"Self-balancing Architectures based on Liquid Functions across Computing Continuums",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"1234473c-24c6-4ba8-8d9b-770b7b503f3c"
        },
        {
            "abstract":"The serverless and functions as a service (FaaS) paradigms are currently trending among cloud providers and are now increasingly being applied to the network edge, and to the Internet of Things (IoT) devices. The benefits include reduced latency for communication, less network traffic and increased privacy for data processing. However, there are challenges as IoT devices have limited resources for running multiple simultaneous containerized functions, and also FaaS does not typically support long-running functions. Our implementation utilizes Docker and CRIU for checkpointing and suspending long-running blocking functions. The results show that checkpointing is slightly slower than regular Docker pause, but it saves memory and allows for more long-running functions to be run on an IoT device. Furthermore, the resulting checkpoint files are small, hence they are suitable for live migration and backing up stateful functions, therefore improving availability and reliability of the system.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.9797970",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2019-08-14",
            "selected":null,
            "title":"Checkpointing and Migration of IoT Edge Functions",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"3766484v-24c6-4ba8-8d9b-770b7b503f3c"
        },
        {
            "abstract":"The serverless technology, introduced for data center operation, represents an attractive technology for latency-sensitive applications operated at the edge, enabling a resource-aware deployment accounting for limited edge computing resources or end-to-end network congestion to the cloud. This paper presents and validates a framework for automated deployment and dynamic reconfiguration of serverless functions at either the edge or cloud. The framework relies on extensive telemetry data retrieved from both the computing and packet-optical network infrastructure and operates on diverse Amazon Web Services technologies, including Greengrass on the edge. Experimental demonstration with a latency-sensitive serverless application is then provided, showing fast dynamic reconfiguration capabilities, e.g., enabling even zero outage time under certain conditions.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.3402024",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2021-08-14",
            "selected":null,
            "title":"Latency-Sensitive Edge/Cloud Serverless Dynamic Deployment Over Telemetry-Based Packet-Optical Network",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"3777773c-24c6-4ba8-8d9b-770b7b503f3c"
        },
        {
            "abstract":"Cloud robotics is becoming an alternative to support advanced services of robots with low computing power as network technology advances. Recently, fog robotics has gained attention since the approach has merit relieving latency and security issues over the conventional cloud robotics. In this paper, a function as a service based fog robotic (FaaS-FR) for cognitive robots is proposed. The model distributes the cognitive functions according to the computational power, latency, and security with a public robot cloud and fog robot server. During the experiment with a Raspberry Pi as an edge, the proposed FaaS-FR model shows efficient and practical performance in the proper distribution of the computational work of the cognitive system.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.9832142",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2019-08-14",
            "selected":null,
            "title":"A Function as a Service Based Fog Robotic System for Cognitive Robots",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"1766473c-24c6-4ba8-8d9b-770b7b503f3c"
        },
        {
            "abstract":"Most studies on network function virtualization are based on cloud computing environments. Fog computing has been proposed as a supplement to cloud computing. When deploying the service function chain (SFC), the consumption of network resources can be effectively reduced by taking advantage of a combination of cloud and fog computing. However, few SFC studies are based on fog computing environments. Moreover, the problem of combining SFCs for the support of live online services to reduce network congestion and save network resources has not been considered. To effectively take advantage of cloud-fog computing and thus achieve the goal of saving resources and reducing network congestion, in this paper, we study the SFC combination and deployment problem in cloud-fog computing environments. To solve this problem, we present an efficient SFC combination and deployment algorithm. Finally, we conduct extensive simulations to evaluate the performance of our proposed algorithm. The results show that our proposed algorithm can effectively reduce network resource consumption and effectively resolve network congestion caused by live online services.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/5021672.9832142",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2018-08-14",
            "selected":null,
            "title":"Towards Resource-Efficient Service Function Chain Deployment in Cloud-Fog Computing",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"1766473a-24c6-4ba8-8d9b-772b7b503f3w"
        },
        {
            "abstract":"Current edge computing frameworks require tight coupling between mobile clients and surrogates, i.e., the offloaded code has been preconfigured with its required execution environment. In many cases, this includes prior transfers of code blocks or execution environments from mobile devices to the offloading infrastructure. This approach incurs additional latency and is detrimental for the energy consumption of the mobile devices. In this paper, we propose the concept of a microservice store. Using the microservice abstraction common in software development and following the serverless paradigm, we envision a repository through which said services are made accessible to developers and can be re-used across applications. We implement a proof-of-concept edge computing system based on a microservice repository and demonstrate its benefits with real-world applications on mobile devices. Our results show that we were able to reduce latencies by up to 14x and save up to 94% of battery life.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/5021672.9868720",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2019-08-14",
            "selected":null,
            "title":"A Microservice Store for Efficient Edge Offloading",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"1766473a-20c6-4ba8-8d9b-681b7b503f3z"
        },
        {
            "abstract":"Systems based on fog computing produce massive amounts of data; accordingly, an increasing number of fog computing apps and services are emerging. In addition, machine learning (ML), which is an essential area, has gained considerable progress in various research domains, including robotics, neuromorphic computing, computer graphics, natural language processing (NLP), decision-making, and speech recognition. Several researches have been proposed that study how to employ ML to settle fog computing problems. In recent years, an increasing trend has been observed in adopting ML to enhance fog computing applications and provide fog services, like efficient resource management, security, mitigating latency and energy consumption, and traffic modeling. Based on our understanding and knowledge, there is no study has yet investigated the role of ML in the fog computing paradigm. Accordingly, the current research shed light on presenting an overview of the ML functions in fog computing area. The ML application for fog computing become strong end-user and high layers services to gain profound analytics and more smart responses for needed tasks. We present a comprehensive review to underline the latest improvements in ML techniques that are associated with three aspects of fog computing: management of resource, accuracy, and security. The role of ML in edge computing is also highlighted. Moreover, other perspectives related to the ML domain, such as types of application support, technique, and dataset are provided. Lastly, research challenges and open issues are discussed.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"01.8567/5021672.9868720",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2019-08-14",
            "selected":null,
            "title":"A Review of Fog Computing and Machine Learning: Concepts, Applications, Challenges, and Open Issues",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"1766473a-20c6-8h5l-8d9b-852b7b503f3z"
        },
        {
            "abstract":"In this paper, we propose a framework for the management of the Internet of Things (IoT) devices in a smart building to model services based on the serverless computing paradigm. The deployment of an IoT compatible serverless paradigm consists of a hierarchical structural design across the edge, fog, and cloud computing layers. The fog/edge nodes collect the data generated from various sensors, process the data in the intermediate nodes, and then forward certain data to a cloud for future analysis. The framework consists of a heterogeneous IoT network. We proposed a data distribution algorithm in the framework to make sure management, maintenance and availability of heterogeneous IoT network in the serverless computing paradigm are effective and efficient. The experiments conducted are validated at the developed fog and edge gateways using API mechanism. The response times for an application doing the computation at fog level and at the cloud level are compared. The experimentation shows that latency is less for the fog model as compared to the data sent to the cloud model.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"05.8567/9841672.9868720",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2020-08-14",
            "selected":null,
            "title":"Serverless Management of Sensing Systems for Fog Computing Framework",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"1766473a-87n6-8h5l-8d9b-852b7b503f3z"
        },
        {
            "abstract":"Cloud native programming and serverless architectures provide a novel way of software development and operation. A new generation of applications can be realized with features never seen before while the burden on developers and operators will be reduced significantly. However, latency sensitive applications, such as various distributed IoT services, generally do not fit in well with the new concepts and today's platforms. In this article, we adapt the cloud native approach and related operating techniques for latency sensitive IoT applications operated on public serverless platforms. We argue that solely adding cloud resources to the edge is not enough and other mechanisms and operation layers are required to achieve the desired level of quality. Our contribution is threefold. First, we propose a novel system on top of a public serverless edge cloud platform, which can dynamically optimize and deploy the microservice-based software layout based on live performance measurements. We add two control loops and the corresponding mechanisms which are responsible for the online reoptimization at different timescales. The first one addresses the steady-state operation, while the second one provides fast latency control by directly reconfiguring the serverless runtime environments. Second, we apply our general concepts to one of today's most widely used and versatile public cloud platforms, namely, Amazon's AWS, and its edge extension for IoT applications, called Greengrass. Third, we characterize the main operation phases and evaluate the overall performance of the system. We analyze the performance characteristics of the two control loops and investigate different implementation options.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"05.8567/6842037.9868720",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2021-08-14",
            "selected":null,
            "title":"Operating Latency Sensitive Applications on Public Serverless Edge Cloud Platforms",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"1766473a-87n6-8h5l-8d9b-890l5b503f3z"
        },
        {
            "abstract":"Serverless Computing and Function as a Service (FaaS) is gaining traction in cloud-based application architectures used by startups and matured organizations alike. Organizations that are keen to leverage modern technology to gain a disruptive edge, optimal efficiency, advanced agility and save cost are adopting these architectural styles rapidly. Cloud service provider offer and dynamically manages the allocation of machine resources in serverless computing. The serverless architectures allows the developers to focus on business logic exclusively without worrying about preparing the runtime, managing deployment and infrastructure related concerns. FaaS may be assumed as a subset of Serverless Computing, in which, instead of coding a full-fledged cloud based application, the developer just writes (often small) functions which are piece of code (in one of the multiple programming languages supported by the platform) dedicated to do a focused, often single task that are invoked by triggers. It offers dynamic allocation and scaling of the resources and innovative trigger based costing model. This paper introduces Serverless Computing, and Function as a Service (FaaS), explores its advantages and limitations, options available with popular cloud and Platform as a Service (PaaS) providers, and emerging use cases and success stories.",
            "authors":["Pelle, I.","Czentye, J."],
            "categories":null,
            "citations":12,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/5897.8521479",
            "keywords":["Greengrass","Lambda","IoT"],
            "number_of_pages":2,
            "pages":"33-34",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380485",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"exemplo"
            },
            "publication_date":"2018-08-10",
            "selected":null,
            "title":"Winning in the era of Serverless Computing and Function as a Service",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115694621&origin=inward"],
            "id":"cc74e78b-4eb1-48d8-9429-8430880947b7"
        },
        {
            "abstract":"Scalable application development is highly influenced by two major trends - serverless computing and continuum computing. These trends have had little intersection, as most application architectures, even when following a microservices or function-based approach, are built around rather monolithic Function-as-a-Service engines that do not span continuums. Functions are thus separated codewise but not infrastructure-wise, as they continue to run on the same single platform they have been deployed to. Moreover, developing and deploying distributed applications remains non-trivial and is a hurdle for enhancing the capabilities of mobile and sensing domains. To overcome this limitation, the concept of self-balancing architectures is introduced in which liquid functions traverse cloud and edge/fog platforms in a continuum as needed, represented by the abstract notion of pressure relief valves based on resource capacities, function execution durations and optimisation preferences. With CoRFu, a reference implementation of a continuum-wide distributed Function-as-a-Service engine is introduced and combined with a dynamic function offloading framework. The implementation is validated with a sensor data inference and regression application.",
            "authors":["Suo, K.","Shi, Y.","Xu, X.","Cheng, D.","Chen, W."],
            "categories":null,
            "citations":2,
            "comments":null,
            "databases":["Scopus"],
            "doi":"10.1145/3405672.8742361",
            "keywords":["Serverless","Performance","Cold Start","Container"],
            "number_of_pages":2,
            "pages":"54-55",
            "publication":{
                "category":"Conference Proceedings",
                "cite_score":null,
                "is_potentially_predatory":false,
                "isbn":"9781450380447",
                "issn":null,
                "publisher":null,
                "sjr":null,
                "snip":null,
                "subject_areas":[],
                "title":"NAI 2020 - Proceedings of the 2020 Workshop on Network Application Integration/CoDesign"
            },
            "publication_date":"2021-08-14",
            "selected":null,
            "title":"Self-balancing Architectures based on Liquid Functions across Computing Continuums",
            "urls":["https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094958728&origin=inward"],
            "id":"1234473c-24c6-4ba8-8d9b-770b7b503f3c"
        }
    ],
    "processed_at":"2023-02-24 00:29:21",
    "publication_types":null,
    "query":"([serverless] OR [function-as-a-service] OR [function as a service] OR [backend-as-a-service] OR [backend as a service] OR [aws lambda] OR [google gloud platform] OR [azure functions]) AND ([fog] OR [edge])",
    "since":"2020-08-06",
    "until":"2020-08-16"
}